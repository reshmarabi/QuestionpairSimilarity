{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUORA QUESTION PAIR SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similarity using methods like   Logistic Regression , xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix  \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"train.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    255024\n",
       "1    149263\n",
       "Name: is_duplicate, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.dropna()\n",
    "df['is_duplicate'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict={\"what's\" : \"what is\" , \"\\'s\" : \" \" , \"\\'ve\" : \" have \" , \"can't\" : \"can not \" , \"n't\" : \" not \" , \"i'm\" : \"i am \" , \"\\'re\" : \" are \" , \"\\'d\" : \" would \" , \"\\'ll\" : \" will \" , \"\\'scuse\" : \" excuse\", '\\'' : \" \", ',' : \" \", '&' : \" \", '#' : \" \", '%' : \" \", \"*\" : \" \", \"(\" : \" \", \")\" : \" \", \"!\" : \" \"}\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        text = text.lower()\n",
    "        for key,values in replace_dict.items():\n",
    "            text=text.replace(key,values)\n",
    "        text = re.sub('\\W', ' ', text)\n",
    "        text = re.sub('\\s+', ' ', text)\n",
    "        text = text.strip(' ')\n",
    "    except Exception as e:\n",
    "        print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question1']=df['question1'].map(lambda x : clean_text(x))\n",
    "df['question2']=df['question2'].map(lambda x : clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(pd.concat((df['question1'],df['question2'])).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "trainq1_trans = tfidf_vectorizer.transform(df['question1'].values)\n",
    "trainq2_trans = tfidf_vectorizer.transform(df['question2'].values)\n",
    "labels = df['is_duplicate'].values\n",
    "X = scipy.sparse.hstack((trainq1_trans,trainq2_trans))\n",
    "y = labels\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.33, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting ...\n",
      "log_loss= 8.30060910565358\n",
      "accuracy= 0.7596746992467114\n",
      "auc  = 0.7214081188494489\n"
     ]
    }
   ],
   "source": [
    "model = linear_model.LogisticRegression(n_jobs=4,solver = 'sag',max_iter = 1000)        \n",
    "model.fit(X_train,y_train)\n",
    "print('predicting ...')\n",
    "y_pred = model.predict(X_valid)\n",
    "loss = log_loss(y_valid,y_pred)\n",
    "print('log_loss= {}'.format(loss))\n",
    "accuracy = accuracy_score(y_valid,y_pred)\n",
    "print('accuracy= {}'.format(accuracy))\n",
    "auc = roc_auc_score(y_valid,y_pred)\n",
    "print('auc  = {}'.format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model using xboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.75712\ttrain-auc:0.872221\n",
      "[1]\teval-auc:0.784094\ttrain-auc:0.90611\n",
      "[2]\teval-auc:0.800946\ttrain-auc:0.928553\n",
      "[3]\teval-auc:0.813536\ttrain-auc:0.945294\n",
      "[4]\teval-auc:0.820934\ttrain-auc:0.953972\n",
      "[5]\teval-auc:0.825845\ttrain-auc:0.961826\n",
      "[6]\teval-auc:0.829639\ttrain-auc:0.965265\n",
      "[7]\teval-auc:0.833585\ttrain-auc:0.97054\n",
      "[8]\teval-auc:0.837491\ttrain-auc:0.974494\n",
      "[9]\teval-auc:0.839403\ttrain-auc:0.97567\n",
      "[10]\teval-auc:0.840988\ttrain-auc:0.976811\n",
      "[11]\teval-auc:0.842983\ttrain-auc:0.97811\n",
      "[12]\teval-auc:0.844122\ttrain-auc:0.978942\n",
      "[13]\teval-auc:0.845806\ttrain-auc:0.979962\n",
      "[14]\teval-auc:0.846923\ttrain-auc:0.980442\n",
      "[15]\teval-auc:0.848045\ttrain-auc:0.98122\n",
      "[16]\teval-auc:0.848928\ttrain-auc:0.981803\n",
      "[17]\teval-auc:0.849865\ttrain-auc:0.982388\n",
      "[18]\teval-auc:0.850661\ttrain-auc:0.982764\n",
      "[19]\teval-auc:0.851406\ttrain-auc:0.983325\n",
      "[20]\teval-auc:0.852749\ttrain-auc:0.984482\n",
      "[21]\teval-auc:0.853486\ttrain-auc:0.984987\n",
      "[22]\teval-auc:0.854138\ttrain-auc:0.985263\n",
      "[23]\teval-auc:0.854641\ttrain-auc:0.985543\n",
      "[24]\teval-auc:0.855279\ttrain-auc:0.985845\n",
      "[25]\teval-auc:0.855856\ttrain-auc:0.986171\n",
      "[26]\teval-auc:0.856434\ttrain-auc:0.986584\n",
      "[27]\teval-auc:0.856877\ttrain-auc:0.986829\n",
      "[28]\teval-auc:0.857175\ttrain-auc:0.987002\n",
      "[29]\teval-auc:0.857705\ttrain-auc:0.987351\n",
      "[30]\teval-auc:0.858008\ttrain-auc:0.987556\n",
      "[31]\teval-auc:0.858796\ttrain-auc:0.987995\n",
      "[32]\teval-auc:0.859118\ttrain-auc:0.98818\n",
      "[33]\teval-auc:0.859547\ttrain-auc:0.988408\n",
      "[34]\teval-auc:0.859884\ttrain-auc:0.988681\n",
      "[35]\teval-auc:0.860519\ttrain-auc:0.989066\n",
      "[36]\teval-auc:0.860757\ttrain-auc:0.989168\n",
      "[37]\teval-auc:0.861174\ttrain-auc:0.989468\n",
      "[38]\teval-auc:0.86148\ttrain-auc:0.98961\n",
      "[39]\teval-auc:0.86197\ttrain-auc:0.989886\n",
      "[40]\teval-auc:0.862258\ttrain-auc:0.990041\n",
      "[41]\teval-auc:0.862853\ttrain-auc:0.990311\n",
      "[42]\teval-auc:0.863117\ttrain-auc:0.990465\n",
      "[43]\teval-auc:0.863454\ttrain-auc:0.990644\n",
      "[44]\teval-auc:0.86369\ttrain-auc:0.990819\n",
      "[45]\teval-auc:0.863963\ttrain-auc:0.990938\n",
      "[46]\teval-auc:0.864248\ttrain-auc:0.991088\n",
      "[47]\teval-auc:0.864471\ttrain-auc:0.991189\n",
      "[48]\teval-auc:0.864675\ttrain-auc:0.991336\n",
      "[49]\teval-auc:0.864906\ttrain-auc:0.991452\n",
      "[50]\teval-auc:0.865261\ttrain-auc:0.991566\n",
      "[51]\teval-auc:0.865482\ttrain-auc:0.991665\n",
      "[52]\teval-auc:0.865598\ttrain-auc:0.991756\n",
      "[53]\teval-auc:0.865789\ttrain-auc:0.991888\n",
      "[54]\teval-auc:0.865928\ttrain-auc:0.991978\n",
      "[55]\teval-auc:0.866094\ttrain-auc:0.99206\n",
      "[56]\teval-auc:0.866632\ttrain-auc:0.992353\n",
      "[57]\teval-auc:0.866852\ttrain-auc:0.992439\n",
      "[58]\teval-auc:0.867212\ttrain-auc:0.992573\n",
      "[59]\teval-auc:0.867361\ttrain-auc:0.992803\n",
      "[60]\teval-auc:0.867476\ttrain-auc:0.992891\n",
      "[61]\teval-auc:0.86772\ttrain-auc:0.992993\n",
      "[62]\teval-auc:0.86788\ttrain-auc:0.993062\n",
      "[63]\teval-auc:0.868015\ttrain-auc:0.993105\n",
      "[64]\teval-auc:0.868137\ttrain-auc:0.993209\n",
      "[65]\teval-auc:0.868331\ttrain-auc:0.99328\n",
      "[66]\teval-auc:0.868545\ttrain-auc:0.993347\n",
      "[67]\teval-auc:0.868649\ttrain-auc:0.9934\n",
      "[68]\teval-auc:0.869024\ttrain-auc:0.993564\n",
      "[69]\teval-auc:0.869217\ttrain-auc:0.993645\n",
      "[70]\teval-auc:0.869398\ttrain-auc:0.993757\n",
      "[71]\teval-auc:0.869704\ttrain-auc:0.993988\n",
      "[72]\teval-auc:0.87093\ttrain-auc:0.99448\n",
      "[73]\teval-auc:0.871029\ttrain-auc:0.994537\n",
      "[74]\teval-auc:0.871157\ttrain-auc:0.994606\n",
      "[75]\teval-auc:0.871319\ttrain-auc:0.994654\n",
      "[76]\teval-auc:0.871457\ttrain-auc:0.994743\n",
      "[77]\teval-auc:0.871812\ttrain-auc:0.994884\n",
      "[78]\teval-auc:0.872074\ttrain-auc:0.994998\n",
      "[79]\teval-auc:0.872296\ttrain-auc:0.995154\n",
      "[80]\teval-auc:0.872497\ttrain-auc:0.995332\n",
      "[81]\teval-auc:0.872692\ttrain-auc:0.995403\n",
      "[82]\teval-auc:0.872873\ttrain-auc:0.995529\n",
      "[83]\teval-auc:0.872949\ttrain-auc:0.995554\n",
      "[84]\teval-auc:0.87313\ttrain-auc:0.995601\n",
      "[85]\teval-auc:0.873352\ttrain-auc:0.995811\n",
      "[86]\teval-auc:0.873489\ttrain-auc:0.99585\n",
      "[87]\teval-auc:0.873608\ttrain-auc:0.995886\n",
      "[88]\teval-auc:0.873807\ttrain-auc:0.995974\n",
      "[89]\teval-auc:0.87395\ttrain-auc:0.996068\n",
      "[90]\teval-auc:0.874273\ttrain-auc:0.996095\n",
      "[91]\teval-auc:0.874388\ttrain-auc:0.996158\n",
      "[92]\teval-auc:0.874502\ttrain-auc:0.99643\n",
      "[93]\teval-auc:0.874576\ttrain-auc:0.996456\n",
      "[94]\teval-auc:0.874764\ttrain-auc:0.996543\n",
      "[95]\teval-auc:0.874841\ttrain-auc:0.996561\n",
      "[96]\teval-auc:0.87501\ttrain-auc:0.996585\n",
      "[97]\teval-auc:0.875078\ttrain-auc:0.9967\n",
      "[98]\teval-auc:0.875219\ttrain-auc:0.996755\n",
      "[99]\teval-auc:0.875342\ttrain-auc:0.996791\n",
      "log_loss= 0.42174581416887885\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train,label = y_train)\n",
    "dvalid = xgb.DMatrix(X_valid,label = y_valid)\n",
    "param = {'max_depth':50, 'eta':0.3, 'silent':1, 'objective':'binary:logistic','subsample':0.8,'gamma':0 }\n",
    "param['nthread'] = 7   \n",
    "param['eval_metric'] = 'auc'\n",
    "num_round = 100\n",
    "evallist  = [(dvalid,'eval'),(dtrain,'train')]\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "y_pred = bst.predict(dvalid)\n",
    "loss = log_loss(y_valid,y_pred)\n",
    "print('log_loss= {}'.format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
